{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu77CmFDbrG_"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1LOwDJdbrHE"
      },
      "source": [
        "Fortunately, with libraries such as Scikit-Learn, it’s now easy to build and use almost any machine learning algorithm. But it’s helpful to have an idea of how a machine learning model works under the hood. This lets us diagnose the model when it’s underperforming or explain how it makes decisions, which is crucial if we want to convince others to trust our models.\n",
        "In this assignment, we’ll look at how to build and use the Decision Tree and the Random Forest in Python. We’ll start by understanding how a single decision tree makes classifications on a simple problem. Then, we’ll work our way to using a random forest on a real-world data science problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCVFoQjQbrHF"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B94-u4U9brHG"
      },
      "source": [
        "The dataset we will use in this assignment is the Sonar dataset.\n",
        "\n",
        "This is a dataset that describes sonar chirp returns bouncing off different surfaces. The 60 predictors are the strength of the returns at different angles. It is a binary classification problem that requires a model to differentiate rocks from metal cylinders. There are 208 observations.\n",
        "\n",
        "It is a well-understood dataset. All of the variables are continuous and generally in the range of 0 to 1. The output variable is a string “M” for mine and “R” for rock, which will need to be converted to integers 1 and 0.\n",
        "\n",
        "By predicting the class with the most observations in the dataset (M or mines) the Zero Rule Algorithm can achieve an accuracy of 53%.\n",
        "\n",
        "You can learn more about this dataset at the UCI Machine Learning repository.\n",
        "https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)\n",
        "\n",
        "Download the dataset for free and place it in the \"data\" folder in your working directory with the filename sonar.all-data.csv."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUKdg4NUbrHH"
      },
      "source": [
        "# Import section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.137523Z",
          "start_time": "2022-01-17T20:27:51.210945Z"
        },
        "id": "zzwCwMaGbrHI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.153537Z",
          "start_time": "2022-01-17T20:27:52.139489Z"
        },
        "id": "wbJBS6LVbrHJ"
      },
      "outputs": [],
      "source": [
        "import tests as tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.169411Z",
          "start_time": "2022-01-17T20:27:52.156443Z"
        },
        "id": "hqHyBtuobrHK"
      },
      "outputs": [],
      "source": [
        "RANDOM_STATE = 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41bt4AafbrHK"
      },
      "source": [
        "# Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntsHbJ6_brHL"
      },
      "source": [
        "Read data and convert targets to integers 1 and 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.216149Z",
          "start_time": "2022-01-17T20:27:52.171403Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "FQGfkMk9brHM",
        "outputId": "3603405a-e408-4a8e-d946-4758001108e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   feat_0  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  \\\n",
              "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
              "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
              "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
              "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
              "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
              "\n",
              "   feat_9  ...  feat_51  feat_52  feat_53  feat_54  feat_55  feat_56  feat_57  \\\n",
              "0  0.2111  ...   0.0027   0.0065   0.0159   0.0072   0.0167   0.0180   0.0084   \n",
              "1  0.2872  ...   0.0084   0.0089   0.0048   0.0094   0.0191   0.0140   0.0049   \n",
              "2  0.6194  ...   0.0232   0.0166   0.0095   0.0180   0.0244   0.0316   0.0164   \n",
              "3  0.1264  ...   0.0121   0.0036   0.0150   0.0085   0.0073   0.0050   0.0044   \n",
              "4  0.4459  ...   0.0031   0.0054   0.0105   0.0110   0.0015   0.0072   0.0048   \n",
              "\n",
              "   feat_58  feat_59  target  \n",
              "0   0.0090   0.0032       0  \n",
              "1   0.0052   0.0044       0  \n",
              "2   0.0095   0.0078       0  \n",
              "3   0.0040   0.0117       0  \n",
              "4   0.0107   0.0094       0  \n",
              "\n",
              "[5 rows x 61 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-05d0bf19-c020-48c6-912b-6df9733d35ff\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feat_0</th>\n",
              "      <th>feat_1</th>\n",
              "      <th>feat_2</th>\n",
              "      <th>feat_3</th>\n",
              "      <th>feat_4</th>\n",
              "      <th>feat_5</th>\n",
              "      <th>feat_6</th>\n",
              "      <th>feat_7</th>\n",
              "      <th>feat_8</th>\n",
              "      <th>feat_9</th>\n",
              "      <th>...</th>\n",
              "      <th>feat_51</th>\n",
              "      <th>feat_52</th>\n",
              "      <th>feat_53</th>\n",
              "      <th>feat_54</th>\n",
              "      <th>feat_55</th>\n",
              "      <th>feat_56</th>\n",
              "      <th>feat_57</th>\n",
              "      <th>feat_58</th>\n",
              "      <th>feat_59</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0200</td>\n",
              "      <td>0.0371</td>\n",
              "      <td>0.0428</td>\n",
              "      <td>0.0207</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>0.0986</td>\n",
              "      <td>0.1539</td>\n",
              "      <td>0.1601</td>\n",
              "      <td>0.3109</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.0159</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0167</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0453</td>\n",
              "      <td>0.0523</td>\n",
              "      <td>0.0843</td>\n",
              "      <td>0.0689</td>\n",
              "      <td>0.1183</td>\n",
              "      <td>0.2583</td>\n",
              "      <td>0.2156</td>\n",
              "      <td>0.3481</td>\n",
              "      <td>0.3337</td>\n",
              "      <td>0.2872</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0.0191</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0262</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>0.1099</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0974</td>\n",
              "      <td>0.2280</td>\n",
              "      <td>0.2431</td>\n",
              "      <td>0.3771</td>\n",
              "      <td>0.5598</td>\n",
              "      <td>0.6194</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0166</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0164</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0078</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0171</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.1098</td>\n",
              "      <td>0.1276</td>\n",
              "      <td>0.0598</td>\n",
              "      <td>0.1264</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0121</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0117</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0762</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0481</td>\n",
              "      <td>0.0394</td>\n",
              "      <td>0.0590</td>\n",
              "      <td>0.0649</td>\n",
              "      <td>0.1209</td>\n",
              "      <td>0.2467</td>\n",
              "      <td>0.3564</td>\n",
              "      <td>0.4459</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0107</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 61 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-05d0bf19-c020-48c6-912b-6df9733d35ff')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-05d0bf19-c020-48c6-912b-6df9733d35ff button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-05d0bf19-c020-48c6-912b-6df9733d35ff');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "PATH = 'data/'\n",
        "df = pd.read_csv(PATH+'sonar-all-data.csv', header=None)\n",
        "df.columns = [f'feat_{col}' if col!=60 else 'target' for col in df.columns]\n",
        "df['target'] = df['target'].map({'M': 1, 'R': 0})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTfK8HvxbrHM"
      },
      "source": [
        "# Split data (train and test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.230836Z",
          "start_time": "2022-01-17T20:27:52.217831Z"
        },
        "id": "c10Xd3BAbrHM"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns='target'), df['target'], test_size=0.2, random_state=RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tdf0IzabrHN"
      },
      "source": [
        "# Homework part 1. Implementation of Random Forest Classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-1lCSNabrHN"
      },
      "source": [
        "## Splitting criteria"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuhjDlAFbrHN"
      },
      "source": [
        "**Task 1 - 2 points.** In this section you should implement two criteria for splitting of the feature space. Any of these can be used in the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DyAUkGcbrHO"
      },
      "source": [
        "### Gini index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeVGu7bfbrHO"
      },
      "source": [
        "0.5 points for *gini_index* function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.246755Z",
          "start_time": "2022-01-17T20:27:52.231795Z"
        },
        "id": "9nPZI7x8brHP"
      },
      "outputs": [],
      "source": [
        "def gini_index(x):\n",
        "    \"\"\" Calculate Gini Index for a node\n",
        "    Args:\n",
        "        x: Numpy-array of targets in a node\n",
        "    Returns:\n",
        "        float: Gini index\n",
        "    \"\"\"   \n",
        "    if len(x) == 0:\n",
        "        return 0.0\n",
        "    else: \n",
        "        data = pd.Series(x)\n",
        "        p = data.value_counts()/data.shape[0]\n",
        "        return 1 - (p**2).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.372197Z",
          "start_time": "2022-01-17T20:27:52.248750Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70rTtR0ibrHP",
        "outputId": "9f3e5eb2-2a23-4b1e-a0fc-4b1c97cddef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m All good!\n"
          ]
        }
      ],
      "source": [
        "tests.test_gini_index(gini_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKp6issPbrHQ"
      },
      "source": [
        "0.5 points for *gini_gain* function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.374411Z",
          "start_time": "2022-01-17T20:27:51.226Z"
        },
        "id": "xSDmObPGbrHQ"
      },
      "outputs": [],
      "source": [
        "def gini_gain(parent_node, splits):\n",
        "    \"\"\" Calculate Gini Gain for a particular split\n",
        "    Args:\n",
        "        parent_node: Numpy-array of targets in a parent node\n",
        "        splits: List of two numpy-arrays. Each numpy-array is targets in a child node\n",
        "    Returns:\n",
        "        float: Gini gain\n",
        "    \"\"\"       \n",
        "    weight_1 = len(splits[0])/(len(splits[0]) + len(splits[1]))\n",
        "    weight_2 = len(splits[1])/(len(splits[0]) + len(splits[1]))\n",
        "    return  gini_index(parent_node) -  weight_1*gini_index(splits[0]) - weight_2*gini_index(splits[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.375408Z",
          "start_time": "2022-01-17T20:27:51.228Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qce83ibpbrHQ",
        "outputId": "19ba9738-7df8-4e93-934e-7148f91cdcc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m All good!\n"
          ]
        }
      ],
      "source": [
        "tests.test_gini_gain(gini_gain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz7NCE5FbrHQ"
      },
      "source": [
        "### Entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWTiQTlvbrHQ"
      },
      "source": [
        "0.5 points for *entropy* function. WARNING! Use only natural logarithm np.log() for calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.376406Z",
          "start_time": "2022-01-17T20:27:51.230Z"
        },
        "id": "Zk2HN7aEbrHR"
      },
      "outputs": [],
      "source": [
        "def entropy(x):\n",
        "    \"\"\" Calculate Entropy for a node\n",
        "    Args:\n",
        "        x: Numpy-array of targets in a node\n",
        "    Returns:\n",
        "        float: Entropy\n",
        "    \"\"\"\n",
        "    # If you use vectorized operations on a vector of frequencies p\n",
        "    # where some of the relative frequencies may be 0, and you need to evaluate\n",
        "    # the expression p * np.log(p), you can use the \"where\" argument of the np.log() function.\n",
        "    # This will leave those elements of the array untouched by the function, \n",
        "    # thus evaluating x*log(x) as zero, exactly as we want it to be.\n",
        "    if len(set(x)) == 1:\n",
        "        return 0.0\n",
        "    else:\n",
        "        data = pd.Series(x)\n",
        "        p = data.value_counts()/data.shape[0]\n",
        "        return np.sum(-p*np.log(np.where(p == 0, 1, p)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.376406Z",
          "start_time": "2022-01-17T20:27:51.233Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQGcYmhwbrHR",
        "outputId": "eab635b2-5eae-4933-fc05-57e7fdef6261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m All good!\n"
          ]
        }
      ],
      "source": [
        "tests.test_entropy(entropy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsJp_7fFbrHR"
      },
      "source": [
        "0.5 points for *information_gain* function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.377403Z",
          "start_time": "2022-01-17T20:27:51.234Z"
        },
        "id": "cUIgfT81brHS"
      },
      "outputs": [],
      "source": [
        "def information_gain(parent_node, splits):\n",
        "    \"\"\" Calculate Information Gain for a particular split\n",
        "    Args:\n",
        "        parent_node: Numpy-array of targets in a parent node\n",
        "        splits: List of two numpy-arrays. Each numpy-array is targets in a child node\n",
        "    Returns:\n",
        "        float: Information Gain\n",
        "    \"\"\"     \n",
        "    weight_1 = len(splits[0])/(len(splits[0]) + len(splits[1]))\n",
        "    weight_2 = len(splits[1])/(len(splits[0]) + len(splits[1]))\n",
        "    return  entropy(parent_node) -  weight_1*entropy(splits[0]) - weight_2*entropy(splits[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.378400Z",
          "start_time": "2022-01-17T20:27:51.236Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIHmcCnibrHS",
        "outputId": "e823f2dc-613f-43e9-9a54-06a8e0aace82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m All good!\n"
          ]
        }
      ],
      "source": [
        "tests.test_information_gain(information_gain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvWW18DabrHS"
      },
      "source": [
        "## Split function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh504mqwbrHS"
      },
      "source": [
        "**Task 2 - 1 point** Implement *split_dataset* function. <br>\n",
        "The rows of the dataframe received by a tree node are split into two dataframes depending on their values in a selected column. Rows with values smaller than the chosen threshold are stored in the resulting left dataframe, and we save rows with values larger than the threshold in the right dataframe. The array with target values should be split in accordance with the feature dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.379398Z",
          "start_time": "2022-01-17T20:27:51.239Z"
        },
        "id": "80-J4pSJbrHT"
      },
      "outputs": [],
      "source": [
        "def split(X, y, value):\n",
        "    \"\"\" Split y-values in order to calculate gain later\n",
        "    Args:\n",
        "        X: 1-dimensional numpy-array of data predictor with shape (N,)\n",
        "        y: 1-dimensional numpy-array of targets with shape (N,)\n",
        "        value (float): the value by which the X should be splitted\n",
        "    Returns:\n",
        "        Two 1-dimensional numpy-arrays with targets related to splits\n",
        "    \"\"\"      \n",
        "    left_mask = X < value\n",
        "    right_mask = X >= value\n",
        "    return y[left_mask], y[right_mask]\n",
        "\n",
        "\n",
        "def split_dataset(X, y, column, value):\n",
        "    \"\"\" Split dataset by a particular column and value\n",
        "    Args:\n",
        "        X: 2-dimensional numpy-array (N, num_feats). N-number of samples\n",
        "        y: 1-dimensional numpy-array of targets with shape (N,)  \n",
        "        column (int): the column by which the X should be splitted\n",
        "        value (float): the value by which the column should be splitted\n",
        "    Returns:\n",
        "        Two 2-dimensional numpy-arrays with data and two 1-dimensional numpy-arrays with targets related to splits\n",
        "        left_X, right_X, left_y, right_y\n",
        "    \"\"\"       \n",
        "    left_mask = X[:,column] < value\n",
        "    right_mask = X[:,column] >= value\n",
        "    left_y, right_y = y[left_mask], y[right_mask]\n",
        "    left_X, right_X = X[left_mask], X[right_mask]\n",
        "    return left_X, right_X, left_y, right_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.380396Z",
          "start_time": "2022-01-17T20:27:51.240Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh-MlPbMbrHT",
        "outputId": "40732f02-ca24-42c0-d0f3-1ac4c8a874ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m All good!\n"
          ]
        }
      ],
      "source": [
        "tests.test_split_dataset(split_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_8sKbpMbrHT"
      },
      "source": [
        "## Decision tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiHAjdbXbrHT"
      },
      "source": [
        "**Task 3 - 4.5 points** In the Tree class below you need to complete the *fit(), predict_row()* and *predict()* functions.<br>\n",
        "Each tree is basically a tree node that may have a left and a right child that are also trees. If a tree doesn't have any child nodes, we call it a leaf.<br>\n",
        "Since features for splitting a dataframe are chosen randomly, we need a mechanism to control this randomness to aquire reproducibility for testing purposes. In our case we use the [Random Generator](https://numpy.org/doc/stable/reference/random/generator.html)<br>\n",
        "By creation of a tree only the **criterion** and the **rng** fields are not filled with Nones, so each tree knows what criterion for dataframe splitting it should use. Each fitted tree knows as well by which column and value the split should be performed. If a tree is a leaf it doesn't have information about the split, but stores the predicted value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T22:03:37.866634Z",
          "start_time": "2022-01-17T22:03:37.816038Z"
        },
        "id": "p8JB83_SbrHT"
      },
      "outputs": [],
      "source": [
        "class Tree(object):\n",
        "    \"\"\"A decision tree classifier.\n",
        "\n",
        "    Args:\n",
        "        criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
        "            The function to measure the quality of a split. Supported criteria are\n",
        "            \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
        "        random_gen: numpy Random Generator object, default=None\n",
        "            Generator should be passed over for reproducible results. If None, \n",
        "            a new np.random.default_rng() will be created.\n",
        "    \"\"\"\n",
        "    def __init__(self, criterion='gini', random_gen=None):\n",
        "        if criterion not in [\"gini\", \"entropy\"]:\n",
        "            raise ValueError(f'Unknown criteria for splits {criterion}')\n",
        "        self.criterion = criterion\n",
        "        self.threshold = None           # value of the next split\n",
        "        self.column_index = None        # column to use for the next split\n",
        "        self.outcome_probs = None       # the predicted value if the tree is a leaf\n",
        "        self.left_child = None\n",
        "        self.right_child = None\n",
        "        self.rng = random_gen if random_gen is not None else np.random.default_rng()\n",
        "\n",
        "    @property\n",
        "    def is_terminal(self):\n",
        "        \"\"\"Define is it terminal node.\"\"\"          \n",
        "        return not bool(self.left_child and self.right_child)\n",
        "    \n",
        "    def _compute_gain(self, y, splits):\n",
        "        \"\"\"Compute split gain according to chosen criteria.\"\"\"\n",
        "        func = gini_gain if self.criterion == 'gini' else information_gain\n",
        "        return func(y, splits)\n",
        "\n",
        "    def _find_splits(self, X):\n",
        "        \"\"\"Find all possible split values.\"\"\"\n",
        "        split_values = set()\n",
        "\n",
        "        # Get unique values in a sorted order\n",
        "        x_unique = list(np.unique(X))\n",
        "        for i in range(1, len(x_unique)):\n",
        "            # Find a point between two values\n",
        "            average = (x_unique[i - 1] + x_unique[i]) / 2.0\n",
        "            split_values.add(average)\n",
        "\n",
        "        return list(split_values)\n",
        "\n",
        "    def _find_best_split(self, X, y, n_features):\n",
        "        \"\"\"Find best feature and value for a split. Greedy algorithm.\"\"\"\n",
        "        # Sample random subset of features\n",
        "        subset = self.rng.choice(list(range(0, X.shape[1])), n_features, replace=False)\n",
        "\n",
        "        max_gain, max_col, max_val = None, None, None\n",
        "\n",
        "        for column in subset:\n",
        "            split_values = self._find_splits(X[:, column])\n",
        "            for value in split_values:\n",
        "                splits = split(X[:, column], y, value)\n",
        "                gain = self._compute_gain(y, splits)\n",
        "\n",
        "                if (max_gain is None) or (gain > max_gain):\n",
        "                    max_col, max_val, max_gain = column, value, gain\n",
        "        return max_col, max_val, max_gain\n",
        "    \n",
        "    def _compute_outcome(self, y):\n",
        "        \"\"\"Save outcome probabilities for the leaf.\"\"\"\n",
        "        self.outcome_probs = np.around(np.sum(y) / y.shape[0])\n",
        "\n",
        "    def fit(self, X, y, feature_frac=1.0, max_depth=None):\n",
        "        \"\"\"Fit model.\n",
        "\n",
        "        Args:\n",
        "            X (numpy-array):      The training input samples. 2-dimensional numpy array.\n",
        "            y (numpy-array):      The target values. 1-dimensional numpy array.\n",
        "            feature_frac (float): The fraction of features to use when fit is performed. Must be \n",
        "                                  between 0.0 and 1.0 (default: 1.0, i.e. all features).\n",
        "            max_depth (int):      The maximum depth of the tree. If None, then nodes are expanded\n",
        "                                  until all leaves are pure.\n",
        "        \"\"\"\n",
        "        # 3 points\n",
        "        \n",
        "        # Make this tree a leaf and return if max_depth doesn't allow it to grow further\n",
        "        if max_depth is not None and None:\n",
        "            pass\n",
        "        \n",
        "        if not 0.0 <= feature_frac <= 1.0:\n",
        "            raise ValueError('feature_frac must lie between 0.0 and 1.0')\n",
        "        # Compute the number of features to use. Should be integer number\n",
        "        n_features = int(None)\n",
        "        \n",
        "        # Find the next split    \n",
        "        column, value, gain = None\n",
        "        \n",
        "        # Make this tree a leaf and return if the split doesn't bring any gain\n",
        "        if gain is None or None:\n",
        "            pass\n",
        "        \n",
        "        # Save the necessary information about the next split\n",
        "        pass\n",
        "        \n",
        "        # Reduce the max_depth parameter for the next tree\n",
        "        if max_depth is not None:\n",
        "            pass\n",
        "\n",
        "        # Split dataset\n",
        "        left_X, right_X, left_target, right_target = None\n",
        "\n",
        "        # Grow left and right child. Pass over the parent's random generator\n",
        "        # for reproducible results \n",
        "        self.left_child = None\n",
        "        pass\n",
        "\n",
        "        self.right_child = None\n",
        "        pass\n",
        "\n",
        "    def predict_row(self, row):\n",
        "        \"\"\"Predict single row.\"\"\"\n",
        "        # 0.5 points\n",
        "        \n",
        "        if not self.is_terminal:\n",
        "            # Use the stored information about the split to define \n",
        "            # if the right or the left child to use\n",
        "            if row[None] < None:\n",
        "                pass\n",
        "            else:\n",
        "                pass\n",
        "        return None\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions.\n",
        "\n",
        "        Args:\n",
        "            X (numpy-array): The test input samples. 2-dimensional numpy array.\n",
        "        Returns:\n",
        "            1-dimensional numpy-array with predictions     \n",
        "        \"\"\" \n",
        "        # 1 point\n",
        "        \n",
        "        # Create a numpy array of zeros of necessary length to save predictions\n",
        "        result = None\n",
        "        # Go through all rows of the dataset\n",
        "        for i in None:\n",
        "            # Save the prediction for the current row\n",
        "            pass\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.381392Z",
          "start_time": "2022-01-17T20:27:51.244Z"
        },
        "id": "mPhfl4aqbrHU"
      },
      "outputs": [],
      "source": [
        "tests.test_tree(Tree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5lRQPKmbrHU"
      },
      "source": [
        "**Task 4 - 1 point** Fit two models with \"max_depth=3\" and \"max_depth=None\" hyperparameters. **Explain** the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.382390Z",
          "start_time": "2022-01-17T20:27:51.246Z"
        },
        "id": "7Oll-k4CbrHU"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.383389Z",
          "start_time": "2022-01-17T20:27:51.248Z"
        },
        "id": "c4ZQiVsIbrHU"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.384384Z",
          "start_time": "2022-01-17T20:27:51.249Z"
        },
        "id": "QvXeLV2_brHV"
      },
      "outputs": [],
      "source": [
        "def plot_metrics_on_depths(criterion='gini'):\n",
        "    max_depths, train_scores, test_scores = [], [], []\n",
        "    for max_depth in range(0, 8):\n",
        "        max_depths.append(max_depth)\n",
        "        model = Tree(criterion=criterion)\n",
        "        model.fit(X_train.values, y_train.values, max_depth=max_depth)\n",
        "        train_scores.append(accuracy_score(y_train, model.predict(X_train.values)))\n",
        "        y_pred = model.predict(X_test.values)\n",
        "        test_scores.append(accuracy_score(y_test, y_pred))\n",
        "    res_metrics = pd.DataFrame(data={'train': train_scores, 'test': test_scores}, index=max_depths)\n",
        "    sns.set(rc={'figure.figsize':(16, 6)})\n",
        "    plt.plot(res_metrics['train'], label='train score')\n",
        "    plt.plot(res_metrics['test'], label='test score')\n",
        "    plt.title(\"Accuracy as a function of max_depth\")\n",
        "    plt.xlabel('max_depth')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend()\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.384384Z",
          "start_time": "2022-01-17T20:27:51.250Z"
        },
        "id": "UiyUo7-XbrHV"
      },
      "outputs": [],
      "source": [
        "plot_metrics_on_depths('gini')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.385383Z",
          "start_time": "2022-01-17T20:27:51.252Z"
        },
        "id": "op_uQtrLbrHV"
      },
      "outputs": [],
      "source": [
        "plot_metrics_on_depths('entropy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqqrMEFqbrHV"
      },
      "source": [
        "## Random forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uijkQF4ybrHV"
      },
      "source": [
        "**Task 5 - 1.5 Points** In the RandomForestClassifier class below you need to complete the *fit* and *predict()* functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T22:16:11.657659Z",
          "start_time": "2022-01-17T22:16:11.627802Z"
        },
        "id": "NQyWZBvebrHV"
      },
      "outputs": [],
      "source": [
        "class RandomForestClassifier(object):\n",
        "    \"\"\"\n",
        "    A random forest classifier.\n",
        "    A random forest is a meta estimator that fits a number of decision tree\n",
        "    classifiers on various sub-samples of the dataset and uses averaging to\n",
        "    improve the predictive accuracy and control overfitting.\n",
        "    \n",
        "    Args:\n",
        "        n_estimators : int, default=10\n",
        "            The number of trees in the forest.\n",
        "\n",
        "        max_depth : int, default=None\n",
        "            The maximum depth of the tree. If None, then nodes are expanded until\n",
        "            all leaves are pure.        \n",
        "\n",
        "        feature_frac : float, default=None\n",
        "            The fraction of features to use when looking for the best split. Must be \n",
        "            between 0.0 and 1.0. If None, then `feature_frac = 1 / sqrt(n_features)`.\n",
        "\n",
        "        criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
        "            The function to measure the quality of a split. Supported criteria are\n",
        "            \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
        "            \n",
        "        random_gen: numpy Random Generator object, default=None\n",
        "            Generator should be passed over for reproducible results. If None, \n",
        "            a new np.random.default_rng() will be created.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_estimators=10, max_depth=None, feature_frac=None, \n",
        "                 criterion=\"entropy\", bootstrap=True, random_gen=None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.bootstrap = bootstrap\n",
        "        self.feature_frac = feature_frac\n",
        "        \n",
        "        if criterion not in [\"gini\", \"entropy\"]:\n",
        "            raise ValueError(f'Unknown criteria for splits {criterion}')\n",
        "        self.criterion = criterion\n",
        "        \n",
        "        self.rng = random_gen if random_gen is not None else np.random.default_rng()\n",
        "        self.trees = [Tree(criterion=self.criterion, random_gen=self.rng) \n",
        "                            for _ in range(n_estimators)]\n",
        "        \n",
        "    def _init_data(self, X, y):\n",
        "        \"\"\"Ensure data are in the expected format.\n",
        "        Ensures X and y are stored as numpy ndarrays by converting from an\n",
        "        array-like object if necessary. \n",
        "        Parameters\n",
        "        Args:\n",
        "            X : array-like\n",
        "                Feature dataset.\n",
        "            y : array-like, default=None\n",
        "                Target values. By default is required, but if y_required = false\n",
        "                then may be omitted.\n",
        "        \"\"\"\n",
        "        self.size = len(X)\n",
        "        \n",
        "        if not isinstance(X, np.ndarray):\n",
        "            self.X = np.array(X)\n",
        "        else:\n",
        "            self.X = X\n",
        "\n",
        "        if not isinstance(y, np.ndarray):\n",
        "            self.y = np.array(y)\n",
        "        else:\n",
        "            self.y = y\n",
        "            \n",
        "    def bootstrap_data(self, size):\n",
        "        return self.rng.integers(size, size=size)\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit model.\n",
        "\n",
        "        Args:\n",
        "            X (numpy-array): The training input samples. 2-dimensional numpy array.\n",
        "            y (numpy-array): The target values. 1-dimensional numpy array.\n",
        "        \"\"\"\n",
        "        # 0.5 points\n",
        "        \n",
        "        if self.feature_frac is None:\n",
        "            self.feature_frac = 1 / np.sqrt(X.shape[1])\n",
        "        elif not 0.0 <= self.feature_frac <= 1.0:\n",
        "            raise ValueError('feature_frac must lie between 0.0 and 1.0')\n",
        "            \n",
        "        self._init_data(X, y)\n",
        "        \n",
        "        # Iterate over all trees in the forest\n",
        "        for tree in None:\n",
        "            if self.bootstrap:\n",
        "                idxs = self.bootstrap_data(self.size)\n",
        "                X = self.X[idxs]\n",
        "                y = self.y[idxs]\n",
        "            else:\n",
        "                X = self.X\n",
        "                y = self.y\n",
        "                \n",
        "            # Fit the tree    \n",
        "            pass\n",
        "            \n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions.\n",
        "\n",
        "        Args:\n",
        "            X (numpy-array): The test data input samples. 2-dimensional numpy array.\n",
        "        \"\"\"\n",
        "        # 1 point\n",
        "        \n",
        "        if not isinstance(X, np.ndarray):\n",
        "            X = np.array(X)\n",
        "\n",
        "        if self.X is not None:\n",
        "            # Create a numpy array of zeros of necessary length\n",
        "            predictions = None\n",
        "            # Go through all rows of the dataset\n",
        "            for i in None:\n",
        "                # Initialize a counter for voting\n",
        "                row_pred = None\n",
        "                # Sum up predictions from all the voters\n",
        "                for tree in None:\n",
        "                    pass\n",
        "                # Compute the mean of the votes    \n",
        "                row_pred = None\n",
        "                # Save the rounded value as the prediction for the current row\n",
        "                pass\n",
        "            return predictions  \n",
        "        else:\n",
        "            raise ValueError(\"You should fit a model before `predict`\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.387491Z",
          "start_time": "2022-01-17T20:27:51.255Z"
        },
        "id": "ZI7-PgiqbrHW"
      },
      "outputs": [],
      "source": [
        "tests.test_random_forest(RandomForestClassifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amlwgmiybrHW"
      },
      "source": [
        "**Task 6 - 1 Point** Fit two models with \"n_estimators=10\" and \"n_estimators=100\" hyperparameters. **Explain** the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.388380Z",
          "start_time": "2022-01-17T20:27:51.257Z"
        },
        "id": "Yrbq12RvbrHW"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.389380Z",
          "start_time": "2022-01-17T20:27:51.259Z"
        },
        "id": "RoMCqk7WbrHW"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAFN8a1XbrHW"
      },
      "source": [
        "Now it's your turn to explore the various parameters of RandomForestClassifier and their influence on model quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pum2UnXNbrHW"
      },
      "source": [
        "# Homework part 2. Random Forest in action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtHbdHxobrHX"
      },
      "source": [
        "_Note_: Consider **accuracy** as main metric of model performance on this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0k5U9mKbrHX"
      },
      "source": [
        "_Note_: For tasks 1 to 5 please use your custom RandomForestClassifier if it is completed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FpfrFYtbrHX"
      },
      "source": [
        "**Task 1 (0.5 points)** Split the dataset into train, test and validation parts (0.6 / 0.2 / 0.2). First two will be used for model hyperparameter tuning whereas the best model quality should be evaluated on validation part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.390375Z",
          "start_time": "2022-01-17T20:27:51.261Z"
        },
        "id": "gr2KKaY1brHX"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ans_3Ra-brHX"
      },
      "source": [
        "### Grid Search and Random Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUH-QcFdbrHX"
      },
      "source": [
        "If you have enough computational power for model training in a reasonable amount of time more sophisticated approach of hyperparameter tuning would be either Grid Search or Random Search.<br>\n",
        "\n",
        "In a nutshell Grid Search allows you to pass through all different combinations of given model parameters and their values and choose the best combination. Whereas Random Search would randomly choose values for given model parameters and evaluate them on test data untill it reaches the specified number of iterations.<br>\n",
        "\n",
        "More information here [Gentle introduction to Grid and Random search](https://medium.com/@senapati.dipak97/grid-search-vs-random-search-d34c92946318) and here [Detailed Explanation with code examples](https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys_zJKKYbrHX"
      },
      "source": [
        "![grid_random_search.png](attachment:grid_random_search.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T10:10:45.984304Z",
          "start_time": "2022-01-17T10:10:45.969369Z"
        },
        "id": "BVZnxDRsbrHX"
      },
      "source": [
        "**Task 3 (1 point)**. Compare your previous results with [GridSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) hyperparameter tuning. You may tune best hyperparameters for forest with several trees and then increase it while measure the quality on validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.391365Z",
          "start_time": "2022-01-17T20:27:51.265Z"
        },
        "id": "zIXjogBYbrHY"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un2OZv7fbrHY"
      },
      "source": [
        "**Task 4 (1 point)**. And finally tune forest hyperparameters with [RandomSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html). Compare results to previous attempts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.392364Z",
          "start_time": "2022-01-17T20:27:51.266Z"
        },
        "id": "_MxeSTMQbrHY"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnUX9JtGbrHY"
      },
      "source": [
        "**Task 5 (0.5 points)**. Tell us about your experience in hyperparameter tuning with the approaches above. What do you think would be the best option for this task and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHHp-SXZbrHY"
      },
      "source": [
        "Your cool ideas here..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx8yU1ZhbrHY"
      },
      "source": [
        "### Desicion tree explained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P1nK7wsbrHY"
      },
      "source": [
        "Remember the [Titanic](https://www.kaggle.com/c/titanic) competition from last week? Wouldn't be a good idea to visualize one of possible desicion-making processes of _survived_ / _dead_ labeling?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8mA9HLgbrHY"
      },
      "source": [
        "**Task 6 (1 point)**. Load titanic dataset, split it into train/test parts, apply simple hyperparameter tuning of [DesicionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) (use one of the approaches above) in order to have **test accuracy more than 0.65**. <br>\n",
        "\n",
        "Draw the best tree decision making process. You may use [sklearn.tree.prot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html).<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T20:27:52.393362Z",
          "start_time": "2022-01-17T20:27:51.269Z"
        },
        "id": "qlfEHprAbrHY"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX_DiRmebrHY"
      },
      "source": [
        "Is it easy to interpret its results? Are you able to explain to a random person why would they survive / die on the titanic?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "235px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "rf_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ans_3Ra-brHX",
        "jx8yU1ZhbrHY"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}